% DO NOT COMPILE THIS FILE DIRECTLY!
% This is included by the other .tex files.

\begin{frame}[t,plain]
\titlepage
\end{frame}

\begin{frame}[t]{Bayesian Inference How-To}
As the title suggests, this session will be {\bf practical}.
\vspace{1cm}

We will study a selection of techniques that you can use to solve any
inference problem that comes your way!
\end{frame}

\begin{frame}[t]{That said...}
It is inevitable that I will state some opinions and philosophies!\\
{\bf We can't help it.}
\end{frame}

\begin{frame}[t]{Python}
Examples will be given in Python, and will use {\tt numpy} for numerical
things (arrays, random number generation), and {\tt matplotlib} for plotting.\\
\vspace{1cm}
Code should work in either Python 2 or 3.
\end{frame}

\begin{frame}[t, fragile]{Python}
All code will assume the following {\tt import} statements.
\begin{minted}[mathescape,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}
  import numpy as np
  import numpy.random as rng
  import matplotlib.pyplot as plt
  import copy
\end{minted}
\end{frame}


\begin{frame}[t, fragile]{Python}
When I code in Python, I have a C++ accent. Here's a habit that might
seem strange:

\begin{minted}[mathescape,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}
  x = 4  # I know this is an integer
  y = 5. # This is a float because of the decimal point.
\end{minted}
\end{frame}


\begin{frame}[t]{Emphasis}
I will try to emphasise the underlying ideas of the methods.\\
I will not
be teaching specific software packages
(e.g. {\it DNest3}, {\it emcee}, {\it JAGS}, {\it MultiNest}, {\it Stan}),
though I may mention them.
\end{frame}

\begin{frame}[t]{Ingredients I}
Bayesian inference need the following inputs:

\begin{itemize}
\setlength{\itemsep}{20pt}
\item A {\bf hypothesis space} describing the set of possible answers to our
question (``parameter space'' in fitting is the same concept).
\item A {\bf prior distribution} $p(\theta)$ describing how plausible
each of the possible solutions is, not taking into account the data.
\end{itemize}
\end{frame}

\begin{frame}[t]{Ingredients II}
Bayesian inference need the following inputs:
\begin{itemize}
\item A {\bf sampling distribution} $p(D | \theta)$ describing our knowledge
about the connection between the parameters and the data.
\end{itemize}

When $D$ is known,
this is a function of $\theta$ called the {\bf likelihood}.
\end{frame}


\begin{frame}[t]{The Posterior Distribution}
The data helps us by changing our prior distribution to the {\bf posterior
distribution}, given by
\begin{eqnarray*}
p(\theta | D) &=& \frac{p(\theta) p(D|\theta)}{p(D)}
\end{eqnarray*}
where the denominator is the normalisation constant, usually called either
the {\bf marginal likelihood} or the {\bf evidence}.
\begin{eqnarray*}
p(D) &=& \int p(\theta)p(D|\theta) \, d\theta.
\end{eqnarray*}

\end{frame}

\begin{frame}[t]{Posterior Distribution vs. Maximum Likelihood}
The practical difference between these two concepts is greater in higher
dimensional problems.
\begin{center}
\includegraphics[scale=0.35]{bayes.pdf}
\end{center}
\end{frame}




\begin{frame}[t]{Updating Probabilities: Example}
\begin{center}
\includegraphics[scale=0.5]{ebola.jpg}
\end{center}
A patient goes to the doctor because he as a fever. Define
\begin{center}
\begin{tabular}{ll}
$H \equiv $ & ``The patient has Ebola''\\
$\bar{H} \equiv $ & ``The patient does not have Ebola''.
\end{tabular}
\end{center}

\end{frame}

\begin{frame}[t]{Updating Probabilities: Example}
Based on all of her knowledge, the doctor assigns probabilities to the two
hypotheses.
\begin{eqnarray*}
P(H) &=& 0.01\\
P(\bar{H}) &=& 0.99
\end{eqnarray*}

But she wants to test the patient to make sure.
\end{frame}



\begin{frame}[t]{Updating Probabilities: Example}
The patient is tested. Define

\begin{center}
\begin{tabular}{ll}
$D \equiv $ & ``The {\bf test says} the patient has Ebola''\\
$\bar{D} \equiv $ & ``The {\bf test says} the patient does not have Ebola''.
\end{tabular}
\end{center}

If the test were perfect, we'd have $P(D | H) = 1$, $P(\bar{D} | H) = 0$,
$P(D | \bar{H}) = 0$, and $P(\bar{D} | \bar{H}) = 1$.
\end{frame}


\begin{frame}[t]{Updating Probabilities: Example}
The Ebola test isn't perfect. Suppose there's a 5\% probability it simply gives
the wrong answer. Then we have:

\begin{eqnarray*}
P(D | H)   &=& 0.95\\
P(\bar{D} | H) &=& 0.05\\
P(D | \bar{H})   &=& 0.05\\
P(\bar{D} | \bar{H}) &=& 0.95
\end{eqnarray*}

\end{frame}

\begin{frame}[t]{Updating Probabilities: Example}
Overall, there are four possibilities, considering whether the patient has
Ebola or not, and what the test says.

\begin{center}
$(H, D)$\\
$(\bar{H}, D)$\\
$(H, \bar{D})$\\
$(\bar{H}, \bar{D})$
\end{center}


\end{frame}

\begin{frame}[t]{Updating Probabilities: Example}
The probabilities for these four possibilities can be found using the product
rule.
\begin{eqnarray*}
P(H, D) &=& 0.01 \times 0.95\\
P(\bar{H}, D) &=& 0.99 \times 0.05\\
P(H, \bar{D}) &=& 0.01 \times 0.05\\
P(\bar{H}, \bar{D}) &=& 0.99 \times 0.95\\
\end{eqnarray*}
\vspace{-45pt}

These four possibilities are {\bf mutually exclusive} (only one of them is true)
and exhaustive (it's not ``something else''), so the probabilities add up to 1.

\end{frame}

\begin{frame}[t]{Updating Probabilities: Example}
The test results come back and say that the patient has Ebola. That is, we've
learned that $D$ is true. So we can confidently rule out those possibilities
where $D$ is false:

\begin{eqnarray*}
P(H, D) &=& 0.01 \times 0.95\\
P(\bar{H}, D) &=& 0.99 \times 0.05\\
{\color{red} P(H, \bar{D})} &=& {\color{red} 0.01 \times 0.05}\\
{\color{red} P(\bar{H}, \bar{D})} &=& {\color{red} 0.99 \times 0.95}\\
\end{eqnarray*}


\end{frame}


\begin{frame}[t]{Updating Probabilities: Example}
We are left with these two possibilities.

\begin{eqnarray*}
P(H, D) &=& 0.01 \times 0.95\\
P(\bar{H}, D) &=& 0.99 \times 0.05
\end{eqnarray*}

It would be strange to modify these probabilities just because we deleted the
other two. The only thing we have to do is renormalise them, by dividing by the total, so they sum to 1 again.
\end{frame}

\begin{frame}[t]{Updating Probabilities: Example}
Normalising, we get

\begin{eqnarray*}
P(H | D) &=& (0.01 \times 0.95)/(0.01 \times 0.95 + 0.99\times0.05) = 0.161\\
P(\bar{H} | D) &=& (0.99 \times 0.05)/(0.01 \times 0.95 + 0.99\times0.05) = 0.839
\end{eqnarray*}
\end{frame}

\begin{frame}[t]{Moral}
Bayesian updating is completely equivalent to:
\begin{itemize}
\item Writing a list of possible answers to your question
\item Giving a probability to each
\item Deleting the ones that you discover are false.
\end{itemize}

It just seems more complicated than this because we often apply it to more
complex sets of hypotheses.
\end{frame}



\begin{frame}[t]{Transit Example}
This example is quite simple, yet it is complex enough to demonstrate many
important principles.
\vspace{1cm}

It is also closely related to many astronomical situations!
\end{frame}


\begin{frame}[t]{Transit Example}
\begin{figure}
\includegraphics[scale=0.4]{Code/transit_data.pdf}
\end{figure}
\end{frame}



\begin{frame}[t]{Related to the transit example...}
\begin{itemize}
\item Realistic exoplanet transits
\item Finding emission/absorption lines in spectra
\item Finding stars/galaxies in an image
\item ¡Y mucho más!
\end{itemize}
\end{frame}


\begin{frame}[t]{Transit Example: The Truth}
The red curve was:
\begin{eqnarray*}
\mu(t) &=& \left\{
\begin{array}{lr}
10, & 2.5 \leq t \leq 4.5\\
5,  & \textnormal{otherwise}.
\end{array}
\right.
\end{eqnarray*}

\begin{figure}
\includegraphics[scale=0.3]{Code/transit_data.pdf}
\end{figure}
\end{frame}

\begin{frame}[fragile, t]{Transit Example: The Truth}
The red curve was:
\begin{eqnarray*}
\mu(t) &=& \left\{
\begin{array}{lr}
10, & 2.5 \leq t \leq 4.5\\
5,  & \textnormal{otherwise}.
\end{array}
\right.
\end{eqnarray*}

and the noise was added like this:
\begin{minted}[mathescape,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}
  # Add noise
  sig = 1.
  y += sig*rng.randn(y.size)
\end{minted}
\end{frame}

\begin{frame}[fragile, t]{Transit Example: Inference}
Let's fit the data with this model:
\begin{eqnarray*}
\mu(t) &=& \left\{
\begin{array}{lr}
A, & (t_c - w/2) \leq t \leq (t_c + w/2)\\
A-b,  & \textnormal{otherwise}.
\end{array}
\right.
\end{eqnarray*}

We don't know $A$, $b$, $t_c$, and $w$. But we do know the data $D$.
\end{frame}



\begin{frame}[fragile, t]{Transit Example: Parameters}
We don't know $A$, $b$, $t_c$, and $w$.
These are our unknown parameters. Let's find the posterior.

\begin{eqnarray*}
p(A, b, t_c, w | D) &=& \frac{p(A, b, t_c, w)p(D | A, b, t_c, w)}{p(D)}
\end{eqnarray*}
\end{frame}

\begin{frame}[fragile, t]{Transit Example: Problems I}
The posterior is given by:
\begin{eqnarray*}
p(A, b, t_c, w | D) &=& \frac{p(A, b, t_c, w)p(D | A, b, t_c, w)}{p(D)}
\end{eqnarray*}

But...\\
How do we choose the {\it prior}, $p(A, b, t_c, w)$?\\
How do we choose the {\it likelihood}, $p(D | A, b, t_c, w)$?\\
How do we find $p(D)$?
\end{frame}


\begin{frame}[t]{Choosing priors}
The prior $p(A, b, t_c, w)$
describes what values are plausible, without taking the data into account.

Using the product rule, we can break this down:
\begin{eqnarray*}
p(A, b, t_c, w) &=& p(A) p(b | A) p(t_c | b, A) p(w | t_c, b, A)
\end{eqnarray*}
Often, we can assume the prior factorises like this (i.e. the priors are
{\bf independent}):
\begin{eqnarray*}
p(A, b, t_c, w) &=& p(A) p(b) p(t_c) p(w)
\end{eqnarray*}


\end{frame}


\begin{frame}[t]{Choosing priors}
Often, before we get the data, we have a lot of uncertainty about the
values of the parameters. That's why we wanted the data!

This motivates {\bf vague priors}.

\end{frame}



\begin{frame}[fragile, t]{Transit Example: Problems II}
Even if we can calculate the posterior $p(A, b, t_c, w | D)$, it is still a
probability distribution over a four-dimensional space.\\
\vspace{20pt}
{\bf How can we understand and visualise it?}
\end{frame}




\begin{frame}[t]{Answer to Problem II: Monte Carlo}
\begin{columns}[T]
\begin{column}{0.35\textwidth}
  \vspace{20pt}
  \begin{itemize}
  \setlength{\itemsep}{10pt}
  \item {\bf Marginalisation} becomes trivial
  \item We can quantify all uncertainties we might be interested in
  \end{itemize}
\end{column}
\hfill
\begin{column}{0.5\textwidth}
  \hspace{-30pt}
  \includegraphics[scale=0.22]{marginalisation.pdf}
\end{column}

\end{columns}
\end{frame}



\begin{frame}[t]{Answer to Problem II: Monte Carlo}
e.g. Posterior mean of $w$:
\begin{eqnarray}
\int w p(A, b, t_c, w | D) \, dA \, db \, dt_c \, dw
\approx \frac{1}{N}\sum_{i=1}^N w_i
\end{eqnarray}
(i.e. just the arithmetic mean). Probability of being in some region $R$:
\begin{eqnarray}
\int_R p(A, b, t_c, w | D) \, dA \, db \, dt_c \, dw
\approx \frac{1}{N}\sum_{i=1}^N \mathds{1}\left(\theta_i \in R \right)
\end{eqnarray}
(i.e. just the fraction of the samples in $R$).


\end{frame}


\begin{frame}[t]{Monte Carlo}
Samples from the posterior are very useful, but how do we generate them?

\begin{center}
{\tt https://www.youtube.com/watch?v=Vv3f0QNWvWQ}
\end{center}

\end{frame}



\begin{frame}[t]{The Metropolis Algorithm}

\begin{itemize}
\item Start at some point $\theta$ in the hypothesis space.
\item Loop\\
$\{$
  \begin{itemize}
  \item Generate {\bf proposal} from some distribution $q(\theta' | \theta)$
  (e.g. slightly perturb the current position).
  \item With probability $\alpha = \min\left(1, \frac{p(\theta')p(D|\theta')}{p(\theta)p(D|\theta)}\right)$, accept the proposal (i.e. replace $\theta$ with $\theta'$).
  \item Otherwise, stay in the same place.
  \end{itemize}
$\}$
\end{itemize}
\end{frame}

\begin{frame}[t]{Acceptance Probability}
The full acceptance probability is

\begin{eqnarray}
\alpha =
\min\left(1, \frac{q(\theta|\theta')}{q(\theta'|\theta)}\frac{p(\theta')}{p(\theta)}\frac{p(D|\theta')}{p(D|\theta)}\right)
\end{eqnarray}
We'll usually make choices where the $q$s cancel out, and sometimes we'll
choose the $q$s to also cancel out the prior ratio (easier than it sounds).
\end{frame}



\begin{frame}[t]{Implementing the Metropolis Algorithm}
To use Metropolis on the Transit Problem, we'll need functions to:
\begin{itemize}
\item Generate a starting point
\item Make proposals
\item Evaluate the prior distribution at any point
\item Evaluate the likelihood at any point
\end{itemize}
\end{frame}


\begin{frame}[t, fragile]{Random Walk Proposals}
\begin{minted}[mathescape,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}
  # Generate a proposal
  L = 1.
  proposal = x + L*rng.randn()
\end{minted}
\end{frame}

\begin{frame}[t, fragile]{Heavy-Tailed Random Walk Proposals}
\begin{minted}[mathescape,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}
  # Generate a proposal
  L = 10.**(1.5 - 6.*rng.rand())
  proposal = x + L*rng.randn()
\end{minted}
\end{frame}


\begin{frame}[t, fragile]{Useful Plots: The Trace Plot}
\begin{minted}[mathescape,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}
  # Trace plot of the first parameter
  plt.plot(keep[:,0])
\end{minted}
\end{frame}

\begin{frame}[t]{Useful Plots: The Trace Plot}
\begin{center}
\includegraphics[scale=0.4]{Code/trace_plot.pdf}
\end{center}
\end{frame}


\begin{frame}[t, fragile]{Useful Plots: Marginal Posterior}
\begin{minted}[mathescape,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}
  # Marginal posterior for first parameter
  # Excluding first 2000 points
  plt.hist(keep[:,0], 100)
\end{minted}
\end{frame}

\begin{frame}[t]{Useful Plots: Marginal Posterior}
\begin{center}
\includegraphics[scale=0.4]{Code/marginal_posterior.pdf}
\end{center}
\end{frame}

\begin{frame}[t]{Comment on Histograms}
If your histograms have so many points that they look perfectly smooth, you
are working on an {\bf easy problem}!
\end{frame}


\begin{frame}[t, fragile]{Useful Plots: Joint Posterior}
\begin{minted}[mathescape,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}
  # Joint posterior for first two parameters
  # excluding first 2000 points
  plt.plot(keep[:,0], keep[:,1], 'b.')
\end{minted}
\end{frame}

\begin{frame}[t]{Useful Plots: Joint Posterior}
\begin{center}
\includegraphics[scale=0.4]{Code/joint_posterior.pdf}
\end{center}
\end{frame}




\begin{frame}[t]{Question}

How do we choose prior distributions?
How do we 

\end{frame}



\begin{frame}[t]{Parameter Estimation}
What is a parameter?

\begin{itemize}
\item A quantity whose value you would like to know; or
\item A quantity you think you need in order to write down
$p(D | \theta)$.
\end{itemize}

The latter are often called {\bf nuisance parameters}.
\end{frame}




