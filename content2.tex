% DO NOT COMPILE THIS FILE DIRECTLY!
% This is included by the other .tex files.

\begin{frame}[t,plain]
\titlepage
\end{frame}

\begin{frame}[t]{Topics for Today}
We will look at:

\begin{itemize}
\item Trans-dimensional MCMC
\item Nested Sampling
\end{itemize}

\end{frame}


\begin{frame}[t]{Trans-dimensional MCMC}
Trans-dimensional MCMC is useful when the model dimension is unknown. This
arises quite frequently in astrophysics.
\end{frame}


\begin{frame}[t]{Trans-dimensional examples}
Some examples from my own work: How many stars are in these images (and what are their positions and fluxes)?

\begin{center}
\includegraphics[scale=0.4]{starfield.pdf}
\end{center}

\end{frame}

\begin{frame}[t]{Trans-dimensional examples}
How many stars were there?

\begin{center}
\includegraphics[scale=0.25]{inference1.pdf}
\end{center}

\end{frame}

\begin{frame}[t]{Trans-dimensional examples}
How many stars were there?
\begin{center}
\includegraphics[scale=0.25]{inference2.pdf}
\end{center}

\end{frame}

\begin{frame}[t]{Trans-dimensional examples}
Perhaps you can see how to use MCMC to estimate the parameters of the
stars ($x$ and $y$ position, and a brightness, for each), if we knew the
number of stars.

\vspace{20pt}
But we want to know the number of stars!
\end{frame}

\begin{frame}[t]{Asteroseismology Example}

\begin{center}
\includegraphics[scale=0.4]{star.png}
Image credit: Tim Bedding
\end{center}

\end{frame}

\begin{frame}[t]{Asteroseismology: Time Series Data}

\begin{center}
\includegraphics[scale=0.22]{betahyi1.png}\\
Image credit: Tim Bedding
\end{center}

\end{frame}

\begin{frame}[t]{Asteroseismology Example: Power Spectrum}

\begin{center}
\includegraphics[scale=0.25]{betahyi2.png}\\
Image credit: Tim Bedding
\end{center}

\end{frame}

\begin{frame}[t]{Asteroseismology Example: Toy Dataset}

\begin{center}
\includegraphics[scale=0.35]{Code/asteroseismology_data.pdf}\\
\end{center}

\end{frame}


\begin{frame}[t]{Asteroseismology Example: Question}

\begin{center}
\includegraphics[scale=0.27]{Code/asteroseismology_data.pdf}
\end{center}

Given this data, how many peaks are there? And what are their parameters
(position, height, width)?

\end{frame}



\begin{frame}[t]{Asteroseismology Example}
Each of the peaks has a ``Lorentzian'' shape
(same as the Cauchy distribution!):
\begin{eqnarray}
m(x) &=& B + \sum_{i=1}^N \frac{A_i}
{\left[1 + \left(\frac{x - c_i}{w_i}\right)^2\right]} 
\end{eqnarray}

$A_i$ = amplitude of $i$th component\\
$c_i$ = center of $i$th component\\
$w_i$ = width of $ith$ component\\


\end{frame}

\begin{frame}[t]{Asteroseismology Example}
The sampling distribution/likelihood is

\begin{eqnarray*}
y_i \sim \textnormal{Exponential}(m(x_i; \theta)).
\end{eqnarray*}

i.e.
\begin{eqnarray*}
p(\{y_i\} | \theta) &=& \prod_{i=1}^n \frac{1}{m(x_i; \theta)}
\exp\left[-\frac{y_i}{m(x_i; \theta)}\right].
\end{eqnarray*}
\end{frame}


\begin{frame}[t]{Asteroseismology Example}
Some simple priors are:

\begin{eqnarray*}
N &\sim& \textnormal{Uniform}\left(\{0, 1, 2, ..., 9, 10\}\right)\\
\log(B) &\sim& \textnormal{Uniform}\left[\log(10^{-3}, \log(10^{3})\right]\\
c_i &\sim& \textnormal{Uniform}\left(x_{\rm min}, x_{\rm max}\right)\\
A_i &\sim& \textnormal{Exponential}(\textnormal{mean=10})\\
\log(w_i) &\sim& \textnormal{Uniform}\left[\log(0.01 x_{\rm range}), \log(x_{\rm range})\right]
\end{eqnarray*}

\end{frame}



\begin{frame}[t]{Recall: Monte Carlo}
\begin{columns}[T]
\begin{column}{0.35\textwidth}
  \vspace{20pt}
  \begin{itemize}
  \setlength{\itemsep}{10pt}
  \item {\bf Marginalisation} becomes trivial
  \item We can quantify all uncertainties we might be interested in
  \end{itemize}
\end{column}
\hfill
\begin{column}{0.5\textwidth}
  \hspace{-30pt}
  \includegraphics[scale=0.22]{marginalisation.pdf}
\end{column}

\end{columns}
\end{frame}


\begin{frame}[t]{Recall: The Metropolis Algorithm}

\begin{itemize}
\item Start at some point $\theta$ in the hypothesis space.
\item Loop\\
$\{$
  \begin{itemize}
  \item Generate {\bf proposal} from some distribution $q(\theta' | \theta)$
  (e.g. slightly perturb the current position).
  \item With probability $\alpha = \min\left(1, \frac{p(\theta')p(D|\theta')}{p(\theta)p(D|\theta)}\right)$, accept the proposal (i.e. replace $\theta$ with $\theta'$).
  \item Otherwise, stay in the same place.
  \end{itemize}
$\}$
\end{itemize}
\end{frame}


\begin{frame}[t]{Trans-Dimensional MCMC}
For problems of unknown dimensionality, the hypothesis space is the union
of several fixed-dimension hypothesis spaces. To do MCMC with these models,
you need a way to move between models with different numbers of components.

\begin{center}
\includegraphics[scale=0.7]{drawing.pdf}
\end{center}

\end{frame}


\begin{frame}[t]{Approaches to Trans-Dimensional MCMC}
There are several approaches:

\begin{itemize}
\item Reversible Jump MCMC (Green, 1995)\\
\item Birth and Death MCMC (Stephens, 2000)
\end{itemize}

\end{frame}


\begin{frame}[t]{Approaches to Trans-Dimensional MCMC}
We will do our MCMC like this:

\begin{itemize}
\item Put 10 components in the model, and do MCMC as usual.
\item Interpret the parameter $N$ as the {\bf number of components that are ``switched on''}
\end{itemize}

\end{frame}


\begin{frame}[t]{Code for the priors}
Let's take a look at the Python code for the priors. Remember, the prior
appears in two places:

\begin{itemize}
\item The function {\tt from\_prior}, which we use to generate a starting point
\item The function {\tt log\_prior}, which calculates the log of the prior
density, which is used to determine the acceptance probability.
\end{itemize}
\end{frame}


\begin{frame}[t]{Code for the likelihood}
Let's take a look at the Python code for the likelihood function.\\

\vspace{20pt}

Note how the calculation of the model curve $m(x)$ only sums over the first
$N$ model components, the ones that are {\bf switched on}.

\end{frame}

\begin{frame}[t]{Label Switching Degeneracy}
Imagine we found a solution with two peaks like this:

\begin{eqnarray*}
\textnormal{Peak 1}: \{A, c, w\} &=& \{5, 3, 2\}\\
\textnormal{Peak 2}: \{A, c, w\} &=& \{3, 7, 1\}
\end{eqnarray*}

Then the following solution is completely equivalent:
\begin{eqnarray*}
\textnormal{Peak 1}: \{A, c, w\} &=& \{3, 7, 1\}\\
\textnormal{Peak 2}: \{A, c, w\} &=& \{5, 3, 2\}
\end{eqnarray*}

\end{frame}

\begin{frame}[t]{Label Switching Degeneracy}
When there are $N$ peaks, the posterior will have $N!$ identical modes,
corresponding to switching the order of the peaks.\\

\vspace{20pt}
We can add a proposal move that switches labels. Since the meaning of the
models is the unchanged, this proposal will always be accepted. 
\end{frame}


\begin{frame}[t]{Label Switching Degeneracy}
The {\tt shuffle} function chooses two switched-on peaks ``at random''
and swaps their parameter values.
\end{frame}

\begin{frame}[t]{Label Switching Degeneracy}
When there are $N$ peaks, the posterior will have $N!$ identical modes,
corresponding to switching the order of the peaks.\\

\vspace{20pt}
We can add a proposal move that switches labels. Since the meaning of the
models is the unchanged, this proposal will always be accepted. 
\end{frame}


\begin{frame}[t]{Label Switching Degeneracy}
The {\tt shuffle} function chooses two peaks ``at random''
and swaps their parameter values.
\end{frame}



\begin{frame}[t]{Label Switching Degeneracy}
Consider the marginal posterior distribution for $x_1$. It will be multimodal,
because of the label-switching issue.\\

\vspace{20pt}
In models like this, we can plot a mixture of the posterior for $x_1$, $x_2$
(when it exists), and so on.
\end{frame}





\begin{frame}[t]{Part II: Nested Sampling}
Nested Sampling is a Monte Carlo method (not necessarily MCMC) that was
introduced by John Skilling in 2004.\\
\vspace{20pt}
It is very popular in astrophysics and has some unique strengths.
\end{frame}


\begin{frame}[t]{Marginal Likelihood}
The {\bf marginal likelihood} is useful for ``model selection''. Consider
two models: $M_1$ with parameters $\theta_1$, $M_2$ with parameters $\theta_2$.
The marginal likelihoods are:
\begin{eqnarray*}
p(D | M_1) &=& \int p(\theta_1 | M_1) p(D | \theta_1, M_1) \, d\theta_1\\
p(D | M_2) &=& \int p(\theta_2 | M_2) p(D | \theta_2, M_2) \, d\theta_2
\end{eqnarray*}

These are the normalising constants of the posteriors, within each model.
\end{frame}


\begin{frame}[t]{Bayesian Model Selection}
If you have the marginal likelihoods, it's easy:

\begin{eqnarray*}
\frac{P(M_1 | D)}{P(M_2 | D)} &=& \frac{P(M_1)}{P(M_2)}
\times \frac{P(D | M_1)}{P(D | M_2)}.
\end{eqnarray*}

\begin{eqnarray*}
\textnormal{(posterior odds)} = \textnormal{(prior odds)} \times \textnormal{(bayes factor)}
\end{eqnarray*}

\end{frame}


\begin{frame}[t]{Challenging features}
Another motivation: standard MCMC methods can get stuck in the following
situations:
\begin{center}
\includegraphics[scale=0.4]{challenges.pdf}
\end{center}
\end{frame}

\begin{frame}{Nested Sampling}
Nested Sampling was built to estimate the marginal likelihood.

But it can also be used to generate posterior samples, and it can potentially
work on harder problems where standard MCMC methods get stuck.
\end{frame}

\begin{frame}[t]{Notation}
When discussing Nested Sampling, we use different symbols:
\begin{eqnarray*}
p(D | M_1) &=& \int p(\theta_1 | M_1) p(D | \theta_1, M_1) \, d\theta_1\\
\end{eqnarray*}
becomes
\begin{eqnarray*}
Z &=& \int \pi(\theta) L(\theta) \, d\theta.
\end{eqnarray*}

$Z$ = marginal likelihood, $L(\theta)$ = likelihood function, $\pi(\theta)$ = prior
distribution.
\end{frame}


\begin{frame}[t]{Nested Sampling}
Imagine we had an easy 1-D problem, with a Uniform(0, 1) prior, and a likelihood
that was strictly decreasing.

\begin{center}
\begin{figure}
	\includegraphics[scale=0.28,clip=true,angle=0]{Patricio/AreaZ.pdf}
\caption{Likelihood function with area Z.}
\end{figure}
\end{center}

\end{frame}


\begin{frame}[t]{Nested Sampling}
The key idea of Nested Sampling: Our high dimensional problem can be mapped
onto the easy 1-D problem. Figure from Skilling (2006):

\begin{figure}
\begin{center}
\includegraphics[scale=0.3]{ns.png}
\end{center}
\end{figure}

\end{frame}


%##########################################################################################################################################

%%##########################################################################################################################################
%\begin{frame}{\small{\textbf{Nested Sampling}}}
%Other methods to estimate the evidence:\\~\\
%\begin{itemize}
%\item [*] Harmonic mean.
%\item [*] Thermodynamic integration.
%\item [*] Steppingstone sampling.
%\end{itemize}
%\end{frame}
%#########################
%%##########################################################################################################################################
%\begin{frame}{\small{\textbf{Nested Sampling}}}
%Consider the \textbf<1,2>{positive} random variable $X$ with probability density function $f(x)$ and distribution function $F(x)$. So

%\begin{align*}
%\text{E}(X) = \int_{0}^{\infty} x f(x) \text{d}x \visible<2>{ \equiv \int_{0}^{\infty} (1- F(x))\text{d}x	}
%\end{align*}

%\end{frame}
%%##########################################################################################################################################
%\begin{frame}{\small{\textbf{Nested Sampling}}}
%\begin{overprint}
%\onslide<1>
%\begin{figure}[]    		
%		\includegraphics[scale=0.28,clip=true,angle=0]{Patricio/CDF1.pdf}
%		\caption{Distribution function for a positive random variable X.}
%\end{figure}
%\onslide<2>
%\begin{figure}[]    		
%		\includegraphics[scale=0.28,clip=true,angle=0]{Patricio/CDF2.pdf}
%		\caption{Distribution function for a positive random variable X.}
%\end{figure}
%\end{overprint}
%\end{frame}
%%##########################################################################################################################################
%\begin{frame}{\small{\textbf{Nested Sampling}}}
%Note that $L(\bm\theta | \bm Y ) > 0$ can be seen as a positive random variable with $\bm \theta \sim \pi(\bm\theta)$
%\visible<2,3>{
%\begin{align*}
%\Longrightarrow Z  = \idotsint \pi(\bm\theta) L(\bm{\theta}|\bm{Y}) \text{d}\bm{\theta} = \text{E}_{\pi}\big(L(\bm{\theta}|\bm{Y})\big) 
%\end{align*}}
%\visible<3>{
%\begin{align*}
% = \int_{0}^{\infty} (1-F(\lambda))\text{d}\lambda
%\end{align*}
%where $\lambda = L(\bm{\theta}|\bm{Y})$ and
%\begin{align*}
%F(\lambda) = \idotsint \limits_{L (\bm{\theta})< \lambda} \pi(\bm\theta) \text{d}\bm\theta
%\end{align*}}
%\end{frame}
%##########################################################################################################################################
\begin{frame}{Nested Sampling $X$}
\vspace{-20pt}
Define
\begin{eqnarray*}
X(L^*) = \int \pi(\theta) \mathds{1}\left(L(\theta) > L^*\right)\, d\theta
\end{eqnarray*}

\vspace{10pt}

$X$ is the {\bf amount of prior probability} with likelihood greater than $L^*$.
Loosely, $X$ is the {\bf volume} with likelihood above $L^*$.\\
Higher $L^* \Leftrightarrow$ lower volume.
\end{frame}
%##########################################################################################################################################
%\begin{frame}{Nested Sampling: Meaning of $X$}
%\begin{overprint}
%\onslide<1>
%	\begin{figure}
%		\includegraphics[scale=0.28,clip=true,angle=0]{Patricio/AreaZ.pdf}
%	\caption{Likelihood function with area Z.}
%	\end{figure}
%	$\mathcal L (0.95) = 0.001$ means that 95\% of draws $\bm\theta$ from the prior distribution $\pi(\bm\theta)$ have likelihoods greater than 0.001.		
%\onslide<2>
%\begin{figure}
%	\includegraphics[scale=0.28,clip=true,angle=0]{Patricio/Posterior.pdf}
%	\caption{Posterior sample.}
%\end{figure}	
%\end{overprint}
%\end{frame}
%%##########################################################################################################################################
\begin{frame}{Numerical Integration}
If we had some points with likelihoods $L_i$, and we knew the corresponding
$X$-values, we could approximate the integral numerically, using the
trapezoidal rule or something similar.
\begin{center}
\includegraphics[scale=0.25,clip=true,angle=0]{Patricio/SampleLike2.pdf}
\end{center}
\end{frame}
%######################################%##########################################################################################################################################
\begin{frame}{Nested Sampling Procedure}
This procedure gives us the likelihood values. \\

\begin{itemize}
	\item Sample $\theta=\{\theta_{1}, \ldots , \theta_{N}\}$ from the prior $\pi(\theta)$.
	\item Find the point $\theta_k$ with the worst
likelihood, and let $L^*$ be its likelihood.
	\item Replace $\theta_{k}$ with a new point from $\pi(\theta)$ but restricted to the region where $L(\theta)>L^*$.
\end{itemize}

Repeat the last two steps many times.
The \textit{discarded points} (the worst one at each iteration) are the output.
\end{frame}


\begin{frame}[t]{Generating the new point}
We need a new point from $\pi(\theta)$ but restricted to the region where $L(\theta)>L^*$. The point being replaced has the worst likelihood, so
{\bf all the other points satisfy the constraint!}
\vspace{20pt}

So we can use one of the other points to initialise an MCMC run, trying to
sample the prior, but rejecting any proposal with likelihood below $L^*$.
See code.
\end{frame}

\begin{frame}[t]{Generating the new point}
There are alternative versions of NS available, such as {\bf MultiNest}, that
use different methods (not MCMC) to generate the new point.\\
\vspace{20pt}

I also have a version of NS called {\bf Diffusive Nested Sampling}, which is
a better way of doing NS when using MCMC. I'm happy to discuss it offline.
\end{frame}


\begin{frame}[t]{Nested Sampling Procedure}
Nested Sampling gives us a sequence of points with increasing likelihoods,
but we need to somehow know their $X$-values!
\end{frame}



%%##########################################################################################################################################
%\begin{frame}[t]{\small{\textbf{Nested Sampling}}}
%\begin{overprint}
%\onslide<1>
%\begin{figure}[]    		
%		\includegraphics[scale=0.28,clip=true,angle=0]{Patricio/SampleLike.pdf}
%		%\caption{}
%\end{figure}
%\onslide<2>
%\begin{figure}[]    		
%		\includegraphics[scale=0.28,clip=true,angle=0]{Patricio/SampleLike1.pdf}
%		\caption{The $\mathcal L$ values }
%\end{figure}
%\onslide<3>
%\begin{figure}[]    		
%		\includegraphics[scale=0.28,clip=true,angle=0]{Patricio/SampleLike2.pdf}
%		\caption{Relationship between $\mathcal L$ and $\xi$ values.}
%\end{figure}
%\end{overprint}
%\end{frame}
%##########################################################################################################################################
\begin{frame}[t]{Estimating the $X$ values}
Consider the simple one-dimensional problem with Uniform(0, 1) prior.\\

\vspace{20pt}
When we generate $N$ points from the prior, the distribution for the $X$-value
of the worst point is Beta$(N, 1)$. So we can use a draw from Beta$(N,1)$ as
a guess of the $X$ value.
\end{frame}

\begin{frame}[t]{Estimating the $X$ values}
Each iteration, the worst point should reduce the volume by a factor that has
a Beta$(N, 1)$ distribution. So we can do this:
\begin{eqnarray*}
X_1 &=& t_1\\
X_2 &=& t_2X_1\\
X_3 &=& t_3X_2\\
\end{eqnarray*}

and so on, where $t_i \sim $Beta$(N,1)$. Alternatively, we can use a simple
approximation.
\end{frame}

%##########################################################################################################################################
\begin{frame}[t]{Deterministic Approximation}
\begin{figure}[]    		
		\includegraphics[scale=0.22,clip=true,angle=0]{Patricio/PriorExplore.pdf}
		\caption{Deterministic approximation. Each iteration reduces
the volume by a factor $\approx e^{-1/N}$. e.g. if $N=5$, the worst likelihood
accounts for about 1/5th of the remaining prior volume.}
\end{figure}
\end{frame}
%##########################################################################################################################################
%\begin{frame}[t]{\small{\textbf{Nested Sampling}}}
%Nested Sampling is given by the following steps:
%\begin{enumerate}
%\item Sample $N$ points $\theta_1, \ldots ,\theta_N$ from the prior;
%\item Initialize $Z=0$ and $X_0=1$;
%\item Repeat for $i=1, \ldots, j$;
%			\begin{description}
%				\item[i)  ] find the lowest likelihood $L_i=L(\theta_{l})$;
%				\item[ii) ] set $X_i=\exp(-i/N)$;
%				\item[iii)] set $w_i= X_{i-1}-X_{i}$;
%				\item[iv)] update $Z= w_{i}L_{i} + Z$; and				
%				\item[v)  ] replace $\theta_l$ by drawing a new point $\theta$ from the prior distribution restricted to $L(\theta)>L_i$.
%			\end{description}
%\end{enumerate}
%\end{frame}
%###########%##########################################################################################################################################
\begin{frame}[t]{Posterior Distribution from Nested Sampling}
The posterior sample can be obtained by assigning weights $W_j$ to the
discarded points:
\begin{align*}
W_{j} = \frac{L_{j} w_{j}}{Z} 
\end{align*}
where $w_{j}=X_{j-1} - X_{j+1}$ is the ``prior weight/width'' associated with the
point. The ``effective sample size'' is given by
\begin{align*}
ESS = \exp \left( - \sum_{j=1}^{m} W_j \log W_j \right)
\end{align*}

\end{frame}

\begin{frame}[t]{Information}
NS can also calculate the {\bf information}, also known as the Kullback-Liebler
divergence from the prior to the posterior.

\begin{eqnarray*}
\mathcal{H} &=& \int p(\theta | D) \log\left[\frac{p(\theta | D)}{p(\theta)}\right]
\, d\theta\\
&\approx& \log\left(
\frac{\textnormal{volume of prior}}{\textnormal{volume of posterior}}
\right)
\end{eqnarray*}
\end{frame}

\begin{frame}[t]{Nested Sampling Code}
I have written a basic implementation of Nested Sampling in Python. Let's
use it on the transit problem and the asteroseismology problem.

\end{frame}

\begin{frame}[t]{Nested Sampling Plots}
\vspace{-10pt}
\begin{center}
\includegraphics[scale=0.32]{ns.pdf}
\end{center}

\end{frame}

\begin{frame}[t]{Nested Sampling Plots}
A necessary but not sufficient condition for everything being okay is that you
see the entire peak in the posterior weights.\\

\vspace{20pt}

If it's not there, you haven't done enough NS iterations. i.e. your parameter
values have lower likelihoods than what is typical of the posterior distribution.
\end{frame}

\begin{frame}[t]{Nested Sampling Plots}
The shape of the log$(L)$ vs. log$(X)$ plot is also informative: if it is
straight for a long time, or concave up at some point, your problem contains
a phase transition, and it's a good thing you used Nested Sampling!
\end{frame}

