\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\data}{\boldsymbol{D}}
\newcommand{\documentname}{chapter}

\author[Brendon J. Brewer]{Brendon J. Brewer\\
Department of Statistics, The University of Auckland}

\chapter{Bayesian inference and computation: a beginner's guide}

\section{Introduction}
Most scientific observations are not sufficient to give us definite answers
to all of our questions. It is rare that we get a data set that {\it totally}
answers all of our questions with certainty. Even if that did happen, we would
quickly move on to other questions.
What a data set usually {\it can} do is make hypotheses more or less plausible,
even if we don't achieve total certainty.
Bayesian inference is a model of this
reasoning process, and also a tool we can use
to make quantitative statements about how much
uncertainty we should have about our conclusions. This takes the mystery out
of data analysis, because we no longer have to come up with a new method
every time we face a new problem. Instead, we simply specify exactly what
information we are going to use, and then compute the results.
In the last two decades, Bayesian inference has become immensely popular in
many fields of science, and astrophysics is no exception. Accessible textbooks
for those with a physics background are those by \citet{gregory} and
\citet{sivia}, and parts of the textbook by \citet{mackay}\footnote{Freely
available online at\\
{\tt http://www.inference.phy.cam.ac.uk/itila/}}.
The online tutorial
by Jake Vanderplas is also useful\footnote{Available online at\\
{\tt http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/}}. For those with a strong statistics background,
I recommend the books by \citet{ohagan} and \citep{gelman}. I also maintain
a set of lecture notes for an undergraduate Bayesian statistics
course\footnote{Available at\\
{\tt http://www.github.com/eggplantbren/STATS331/}}.
The aim of this chapter is to present a fairly minimal yet widely applicable set of techniques to
allow you to start using Bayesian inference in your own research.

Any particular application of Bayesian inference involves making choices
about what data you are analysing, what questions you are
trying to answer, and what assumptions you are willing to make.
Data analysis problems in astronomy vary widely, so in this \documentname~we
cannot cover a huge variety of examples. Instead, we will look at a small
number of examples. The assumptions we make in those examples will not always
be appropriate, but they should be sufficient to show you where the assumptions
enter and what you will need to consider when you work on a particular problem.

In principle, it's usually best to work with your data in the most
raw form possible, although this is often too difficult in practice.
Therefore, most scientists work with data that has been processed (by a ``pipeline'') and reduced to manageable size. To do Bayesian inference, you
need to specify what {\it prior information} you have (or are willing to
assume) about the problem, in addition to the data. Prior information is
necessary: what you can learn from a data set depends on what you know, for
example, about how it was produced.

Once you have your data, and have specified your prior information as well,
you are faced with the question of how to calculate the
results. Usually you want to calculate the {\it posterior distribution} for some
unknown quantities (also known as ``parameters'') given your data, which
describes your uncertainty about the , but takes into account the data.
Since we are often dealing with (potentially) complicated probability
distributions in high dimensional spaces, we need to {\it summarise} the
posterior distribution in an understandable way.
In certain problems, the summaries can be calculated analytically, but
numerical methods are more general, so will be the focus of this chapter.
The most popular and useful numerical techniques are the Markov Chain Monte
Carlo methods, often abbreviated as
MCMC\footnote{Some people claim that MCMC stands for
Monte Carlo Markov Chains, but they are wrong.}.
The rediscovery of MCMC in the 1990s
is one of the main reasons why Bayesian inference is so popular now.
While there were many strong philosophical arguments in favour of a Bayesian
approach before then, many people were still uncomfortable with the
subjective elements involved. However, once MCMC made it easy to compute the
consequences of Bayesian models more easily, people simply became more relaxed
about these subjective elements.

A huge variety of MCMC methods exists, and it would be
unwise to try to cover them all in this winter school. Therefore I will focus
on a small number of methods that are relatively simple to implement, yet quite powerful
and widely applicable. I will try to emphasise methods that are {\it general},
i.e. methods that will work okay on most problems you might encounter. One
disadvantage of this approach is that the methods we cover are not necessarily
the most efficient methods possible. If you're mostly interested in one
specific application, you'll probably be able to achieve better performance
by using a more sophisticated algorithm, or by taking advantage of the particular
properties of your problem.
There are many popular software packages (and many more unpopular ones)
available for doing Bayesian Inference, such as JAGS (Just Another Gibbs Sampler),
Stan, emcee, MultiNest, my own DNest3, and many more.
I won't be teaching you how to use any of these programs.
If you or your collaborators already use one of them, that's a great reason to
learn it. Please see the appendix for a brief discussion of the advantages and
disadvantages of some of these packages.

%JAGS is extremely convenient to use

\section{Python}
Due to its popularity and relatively shallow learning curve,
I have implemented the algorithms in this chapter in the Python language.
The code is written so that it works in either Python 2 or 3.
The programs make use of the common numerical library {\tt numpy}, and also
the plotting package {\tt matplotlib}. Any Python code snippets in this chapter
will assume that the packages have been imported like so:

\begin{minted}[mathescape,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}
  import numpy as np
  import numpy.random as rng
  import matplotlib.pyplot as plt
  import copy
\end{minted}



\section{Parameter Estimation}
Almost all data analysis problems can be interpreted as {\it parameter
estimation} problems. The term {\it parameter} has a few different meanings,
but you can usually just think of it as a synonym for {\it unknown quantity}.
When you learned how to solve equations in high school algebra,
you would have been able to find the value of an unknown quantity
(often called $x$) when you had
enough information to determine its value with certainty.
In science, we almost never have enough information to determine a quantity
without any uncertainty, which is why we need probability theory and Bayesian
inference.

We'll denote our unknown parameters by $\btheta$, which could be a single
parameter or perhaps a vector of parameters (e.g. the distance to a star
and the angular diameter of the star). To start, we
need to have some idea of the set of possible values we are considering. For
example, are the parameters integers? Real numbers? Positive real numbers?
In some examples, the definition of the parameters already restricts the set
of possible values. For example, {\it the proportion of extra-solar planets in
the Milky Way that contain life} cannot be less than 0 or greater than 1.
Strictly speaking, it has to be a rational number, but it probably won't make
much difference if we just say it's a real number between 0 and 1 (inclusive).
The distance to a star (measured in whatever units you like) is presumably a
positive real number, as is its angular diameter.
The set of possible values you're willing to consider is called the
{\it hypothesis space}.

To start using Bayesian inference, you need to assign a {\it probability
distribution} on the hypothesis space, which models your initial uncertainty
about the parameters. This probability distribution is called the {\it prior}.
We then use Bayes' rule, a consequence of the product rule of probability,
to calculate the {\it posterior} distribution, which describes our updated
state of knowledge about the values of the parameters, after taking the data
$D$ into account.

For a prior distribution $p(\btheta | I)$, and a sampling
distribution $p(\data | \btheta, I)$, Bayes' rule allows us to calculate
the {\it posterior distribution} for $\btheta$:
\begin{eqnarray}
p(\btheta | \data, I) &=& \frac{p(\btheta | I)p(\data | \btheta, I)}{p(\data | I)}.\label{eq:bayes}
\end{eqnarray}
The $I$ in this equation refers to background information and assumptions:
basically, it stands for everything you know about the problem apart from the
data. The $I$ appears in the background of all of the terms in
Equation~\ref{eq:bayes}, and is often omitted. Note also that the notation
used in Equation~\ref{eq:bayes} is highly simplified but conventional among
Bayesians; see the appendix for a discussion of notation.

The result is a probability distribution for $\btheta$ which describes
our state of knowledge about $\btheta$ after taking into account the data.
The denominator, since it doesn't depend on $\btheta$, is a normalizing
constant, usually called the {\it marginal likelihood} or alternatively the
{\it evidence}. Since the posterior is a probability distribution, its
total integral (or sum, if the
hypothesis space is discrete) must equal 1. Therefore we can write the
marginal likelihood as:

\begin{eqnarray}
p(\data | I) &=& \int p(\btheta | I)p(\data | \btheta, I) \, d^N \btheta.
\end{eqnarray}

The posterior distribution is usually narrower than the prior distribution,
indicating that we have learnt something from the data, and our uncertainty
about the value of the parameters has decreased. See
Figure~\ref{fig:bayes} for an example of the qualitative behaviour we usually
see when updating from a prior distribution to a posterior distribution.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{bayes.pdf}
\caption{An example prior distribution for a single parameter (blue)
gets updated to the posterior distribution (red) by the data. The data
enters through the likelihood function (cyan dotted line). Many traditional
``best fit'' methods are based on finding the {\it maximum likelihood}
estimate, which is the peak of the likelihood function, here denoted by a
star.
\label{fig:bayes}}
\end{center}
\end{figure}

\section{Transit Example}
To become more familiar with Bayesian calculations, we will work through a
simple curve-fitting example. Many astronomical data analysis problems can be
considered as examples of curve fitting.

Consider a transiting exoplanet, like
one observed by {\it Kepler}. The light curve of the star will show an
approximately constant brightness as a function of time, with a small dip as
the exoplanet moves directly in front of the star. Clearly, real Kepler data
is much more complex than this example, as stars vary in brightness in
complicated ways, and the shape of the transit signal itself is more complex
than the model we'll use here. Nevertheless, this example contains many of the
features and complications that will also arise in a more realistic analysis.

The data set, along with the true curve, is shown in 
Figure~\ref{fig:transit_data}. The equation for the true curve was:
\begin{eqnarray*}
\mu(t) &=& \left\{
\begin{array}{lr}
10, & 2.5 \leq t \leq 4.5\\
5,  & \textnormal{otherwise}.
\end{array}
\right.
\end{eqnarray*}

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{../Code/transit_data.pdf}
\caption{The ``transit'' data set. The red curve shows the model prediction
based on the true values of the parameters, and the blue points are the
noisy measurements.\label{fig:transit_data}}
\end{center}
\end{figure}

Let's assume that we didn't know the equation for the true curve,
but we at least know it's a function of the following form:
\begin{eqnarray*}
\mu(t) &=& \left\{
\begin{array}{lr}
A, & (t_c - w/2) \leq t \leq (t_c + w/2)\\
A-b,  & \textnormal{otherwise}.
\end{array}
\right.
\end{eqnarray*}

Thus, the problem has been reduced from not knowing the true curve $\mu(t)$
to not knowing the values of four quantities (parameters)
$A$, $b$, $t_c$, and $w$. Applying Bayesian inference to this problem
involves calculating the posterior distribution for $A$, $b$, $t_c$, and $w$,
given the data $D$. For this specific setup, Bayes' rule states:

\begin{eqnarray}
p(A, b, t_c, w | D) &=& \frac{p(A, b, t_c, w)p(D | A, b, t_c, w)}{p(D)}
\end{eqnarray}

So, in order for the posterior distribution to be well defined, we need to
choose a prior distribution $p(A, b, t_c, w)$ for the parameters, and a
sampling distribution $p(D | A, b, t_c, w)$ for the data. Since the denominator
$p(D)$ is not a function of the parameters, it plays the role of a normalising
constant that ensures the posterior distribution integrates to 1, as any
probability distribution must.

\subsection{Sampling Distribution}
The {\it sampling distribution} is the probability distribution would assign
for the data, if we knew the true values of the parameters.
In many situations, it is conventional to assign a normal distribution
(also known as a gaussian distribution) to each data point, where the mean
of the normal distribution is the noise-free model prediction, and the
standard deviation of the normal distribution is given by the size of the
error bar. Later, we will see how to relax these assumptions in a useful way.
The probability density for the data given the parameters (i.e. the sampling
distribution) is:

\begin{eqnarray}
p(D | A, b, t_c, w) &=& \prod_{i=1}^N \frac{1}{\sigma_i \sqrt{2\pi}}
\exp\left[-\frac{1}{2\sigma_i^2}\left(D_i - \mu(t_i)\right)^2\right]\label{eq:gaussian}
\end{eqnarray}
This is a product of $N$ terms, one for each data point. We have assumed that
each data point is {\it independent} (given the parameters). That is, if we
knew the parameters and some of the data points, in predicting other data points
we would be able to make use of the parameters, but not the data points we
already knew. When the data is known, Equation~\ref{eq:gaussian} becomes a
function of the parameters only: the likelihood function. The curve predicted
by the model, here written as $\mu(t_i)$ (where I have suppressed the implicit
dependence on the parameters, for brevity), provides the mean of the gaussian
distribution. It is important to remember that the independence assumption we
used here is not an assumption about the actual data set, but an assumption
about our prior information about the data set.

Equation~\ref{eq:gaussian} is the sampling distribution
(and the likelihood function) for our problem, but it
was fairly cumbersome to write down. Statisticians have developed a shorthand
notation for writing down probability distributions, which is extremely useful
for communicating what assumptions you are making, without having to write down
the entire equation for the probability density. To communicate
Equation~\ref{eq:gaussian}, we can simply write

\begin{eqnarray}
D_i \sim \mathcal{N}\left(\mu(t_i), \sigma_i^2\right)
\end{eqnarray}
i.e. each data point has a normal distribution (denoted by $\mathcal{N}$)
with mean $\mu(t_i)$ (which
depends on the parameters) and standard deviation $\sigma$. For the normal
distribution, it is traditional to write the {\it variance} (standard deviation
squared) as the second argument, but since the standard deviation is a more
intuitive quantity (being in the same units as the mean), we often literally
write the standard deviation, squared (e.g. $3^2$). For other probability
distributions the arguments in the parenthesis are whatever parameters make
sense for that family of distributions.

\subsection{Priors}
Now we need a prior for the unknown parameters $A, b, t_c$, and $w$. This is
a prior over a four dimensional parameter space. To simplify things, we can
assign independent priors for each parameter, and multiply these together
to produce the joint prior:
\begin{eqnarray}
p(A, b, t_c, w) &=& p(A)p(b)p(t_c)p(w).
\end{eqnarray}
This prior distribution models our uncertainty about the parameters before
taking into account the data. The independence assumption implies that if we
were to learn the value of one of the parameters, this wouldn't tell us anything
about the others. This may or may not be realistic in a given application, but
it is a useful starting point.

Another useful starting point for priors is the uniform distribution.
\begin{eqnarray}
A &\sim& U(-100, 100)\\
b &\sim& U(0, 10)\\
t_c &\sim& U(t_{\rm min}, t_{\rm max})\\
w &\sim& U(0, t_{\rm max} - t_{\rm min})
\end{eqnarray}
The full expression for the probability density is:
\begin{eqnarray}
p(A, b, t_c, w) &=&
\left\{
\begin{array}{lr}
\frac{1}{2000\left(t_{\rm max} - t_{\rm min}\right)^2}, & (A, b, t_c, w) \in S\\
0, & \textnormal{otherwise}
\end{array}
\right.
\end{eqnarray}
where $S$ is the set of allowed values. Even more simply, we can ignore the
normalising constant and the prior boundaries and just write
\begin{eqnarray}
p(A, b, t_c, w) &\propto& 1.
\end{eqnarray}

Now that we have specified our assumed prior information in the form of a
sampling distribution and a prior, we are ready to go. By Bayes' rule, we
have an expression for the posterior distribution immediately:
\begin{eqnarray}
p(A, b, t_c, w | D) &\propto& p(A, b, t_c, w)p(D | A, b, t_c, w)
\end{eqnarray}
which is proportional to the prior times the likelihood. In the likelihood
expression we would substitute in the actual observed dataset, so that it is
a function of the parameters only.
The main problem with using Bayes' rule this way is that a mathematical
expression for a probability distribution in a four-dimensional space is not
very easy to understand intuitively. For this reason, we usually calculate
summaries of the posterior distribution. The main computational tool for doing
this is Markov Chain Monte Carlo.

\section{Markov Chain Monte Carlo}
{\it Monte Carlo} methods allow us to calculate any property of a probability
distribution that is an expectation value. For example, if we have a
single variable
$x$ with a probability density $p(x)$, the expected value is
\begin{eqnarray}
\mathds{E}(x) &=& \int_{-\infty}^{\infty} x p(x) \, dx 
\end{eqnarray}
which is a measure of the ``center of mass'' of the probability distribution.

If we had a set of points $\{x_1, x_2, ..., x_N\}$ ``sampled from'' $f(x)$,
we could replace the integral with a simple average:
\begin{eqnarray}
\mathds{E}(x) &\approx& \frac{1}{N} \sum_{i=1}^N x_i.
\end{eqnarray}
In one dimension, this may not seem very useful. Evaluating a one dimensional
integral analytically is often possible, and doing it numerically using the
trapezoidal rule (or a similar approximation) is quite straightforward.
However, Monte Carlo really becomes useful in higher dimensional problems.
For example, consider a problem with five unknown quantities with probability
distribution $p(a, b, c, d, e)$, and suppose we want to know the probability
that $a$ is greater than $b + c$. We could do the integral
\begin{eqnarray}
P(a > b + c) &=& \int p(a, b, c, d, e) \mathds{1}\left(a > b + c\right) \, da \, db \, dc \,dd \,de 
\end{eqnarray}
where $\mathds{1}\left(a > b + c\right)$ is a function that is equal to one
where the condition is satisfied and zero where it isn't. However, if we could
obtain a sample of points in the five dimensional space, the Monte Carlo
estimate of the probability is simply
\begin{eqnarray}
P(a > b + c) &\approx& \frac{1}{N}\sum_{i=1}^N \mathds{1}\left(a_i > b_i + c_i\right)
\end{eqnarray}
which is just the fraction of the samples that satisfy the condition.

Another important use of Monte Carlo is marginalisation. Suppose again we had
a probability distribution for five variables, but we only cared about one of
them. For example, the marginal distribution of $a$ is given by
\begin{eqnarray}
p(a) &=& \int p(a, b, c, d, e) \, db \, dc \, dd \, de
\end{eqnarray}
which describes your uncertainty about $a$, rather than your uncertainty about
all of the variables. This integral might be analytically intractable. With
Monte Carlo, if you have samples in the five dimensional space but you only
look at the first coordinate, then you have samples from $p(a)$. This is
demonstrated graphically in Figure~\ref{fig:marginalisation}.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{../marginalisation.pdf}
\caption{An example posterior distribution for two parameters $a$ and $b$.
Figure taken from ...
\label{fig:marginalisation}}
\end{center}
\end{figure}

In Bayesian inference, the most important probability distribution is the
posterior distribution for the parameters. We would like to be able to
generate samples from the posterior, so we can compute probabilities,
expectations, and other summaries easily. Markov Chain Monte Carlo allows us
to generate these samples.

\subsection{The Metropolis Algorithm}
The Metropolis-Hastings algorithm, also known as the Metropolis algorithm, is
the oldest and most fundamental MCMC algorithm.
It is quite straightforward to implement, and works well on many problems.
The version of the Metropolis algorithm presented here is sometimes called
{\it random walk} Metropolis. More sophisticated choices are possible,
but usually require problem-specific knowledge.

Consider a problem with a single unknown
parameter $x$. If the prior is some density $\pi(x)$ and the likelihood
function is $\mathcal{L}(x)$, then the posterior distribution will be
proportional to $\pi(x)\mathcal{L}(x)$. The normalising constant
$Z = \int \pi(x) \mathcal{L}(x) \, dx$ is unknown,
but the Metropolis algorithm doesn't need to know it: all we need is the
ability to evaluate $\pi$ and $\mathcal{L}$ at a given position in the parameter space.

The Metropolis algorithm can be summarised as follows:
\begin{itemize}
\item Choose a starting position $x$, somewhere in the parameter space.
\item Generate a proposed position $x'$ from a proposal distribution
$q(x'|x)$.
\item With probability
$\alpha=\min\left(1, \frac{q(x|x')}{q(x'|x)}\frac{\pi(x')}{\pi(x)}\frac{\mathcal{L}(x')}{\mathcal{L}(x)}\right)$, accept the proposal
(i.e. replace $x$ with $x'$). Otherwise, do nothing.
\end{itemize}

The ``random walk'' proposal generates a proposed value $x'$ from a normal
(gaussian) distribution centered around the current position $x$. The user
is free to choose the width of the normal distribution. Here is a Python code
snippet showing a proposal with width {\tt L}:

\begin{minted}[mathescape,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}
  # Generate a proposal
  proposal = x + L*rng.randn()
\end{minted}

The performance of the Metropolis algorithm depends quite strongly on
the width of the proposal distribution. If the width is too small, most moves
will be accepted, but won't move very far. If the width is too large, most
moves will be rejected, so you'll end up stuck in one place. Some authors
recommend using preliminary runs to find an optimal width. Instead, I recommend
that you use a mixture of widths. Basically, every time we make a proposal,
the width is drawn from some range, rather than being constant. The biggest
possible width we would ever want should be roughly the order of magnitude of
the width of the prior (since the posterior is usually narrower than the
prior).
\begin{minted}[mathescape,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}
  # A heavy-tailed proposal distribution
  # Generate a standard deviation
  L = 10.**(1.5 - 6.*rng.rand())

  # Use the standard deviation for the proposal
  proposal = x + L*rng.randn()
\end{minted}
With this proposal, the minimum width is
$10^{-4.5} \approx 3.16 \times 10^{-5}$, and the maximum width is
$10^{1.5} \approx 31.6$. The effective proposal distribution is now very heavy-tailed. As long as a good width is somewhere within our range, things should be
okay.

When there are multiple parameters (almost always, since MCMC isn't necessary
on single parameter problems!), we need to decide how to construct the proposal.
There are two main ways to do this. The first is that the proposal is to
change all of the parameters simultaneously. This tends to be inefficient in
high dimensions, because the only proposals that are likely to be accepted
are those that change the parameter values only slightly: in a high dimensional
space, there are many bad directions to travel, and not very many good ones.
Usually, it's better to propose to change a subset of the parameters, or even
just a single parameter. A python function that takes a {\tt numpy} array of
parameters and input and returns a proposed value for the parameters is
specified below.

\begin{minted}[mathescape,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}
  def proposal(params):
    """
    Generate new values for the parameters.
    The proposal for the Metropolis algorithm.
    """
    # Copy the parameters
    new = copy.deepcopy(params)

    # Which one should we change?
    which = rng.randint(num_params)
    new[which] += jump_sizes[which] \
                    *10.**(1.5 - 6.*rng.rand())*rng.randn()
    return new
\end{minted}
This function relies on {\tt num\_params} being the number of parameters,
and an array {\tt jump\_sizes}, of length {\tt num\_params}, which specifies
the prior width for each parameter.

For numerical reasons, instead of writing a function to evaluate the likelihood
(and the prior density) at a particular point in parameter space, we usually
deal with the logarithms of these quantities. For the transit example, the
log prior function can be implemented like so:

\begin{minted}[mathescape,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}
  def log_prior(params):
    """
    Evaluate the (log of the) prior distribution
    """
    A, b, tc, width = params[0], params[1], params[2], params[3]

    # Minus infinity, if out of bounds
    if A < -100. or A > 100.:
      return -np.Inf
    if b < 0. or b > 10.:
      return -np.Inf
    if tc < t_min or tc > t_max:
      return -np.Inf
    if width < 0. or width > t_range:
      return -np.Inf

    return 0.
\end{minted}

Since we chose uniform priors, the prior density is some constant if the
parameters (here passed to the function as a numpy {\tt array} of four
floating point values) are within the bounds of the uniform priors. Otherwise,
the prior density is zero. We do not need to know the normalising constant
of the prior density: we returned zero for the log-density, but the progress
of the Metropolis algorithm would be the same if we had returned any other
finite value, since the Metropolis algorithm only ever uses ratios of densities
(i.e. differences in log-density).

The log likelihood function is given below.

\begin{minted}[mathescape,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}
  def log_likelihood(params):
    """
    Evaluate the (log of the) likelihood function
    """
    # Rename the parameters
    A, b, tc, width = params[0], params[1], params[2], params[3]

    # First calculate the expected signal
    mu = A*np.ones(N)
    mu[np.abs(data[:,0] - tc) < 0.5*width] = A - b

    # Normal/gaussian distribution
    return -0.5*N*np.log(2.*np.pi) - np.sum(np.log(data[:,2])) \
        -0.5*np.sum((data[:,1] - mu)**2/data[:,2]**2)
\end{minted}



\subsection{Useful Plots}
After running the Metropolis algorithm (or any other MCMC method), there are
several useful plots that you should make. The first is known as a
``trace plot'' (Figure~\ref{fig:trace_plot}),
and is just a plot of one parameter over time as the algorithm run. Trace plots
are the single most useful diagnostic of whether your algorithm is working well.
In the attached code, the MCMC results are stored in a two dimensional
{\tt numpy} array called {\tt keep}, each row of which is the parameter vector
at a particular iteration. It is trivial to make a trace plot from this output:

\begin{minted}[mathescape,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}
  # Trace plot of parameter zero
  plt.plot(keep[:,0])
\end{minted}
The result is shown in Figure~\ref{fig:trace_plot}. When everything is working
well, a trace plot should look like white noise when zoomed out. If this is the
case, then things are {\it probably} working well (although you can never be
100\% certain of this -- like optimisation methods, MCMC methods can get
stuck in local maxima). In addition, if your MCMC run was initialised at a
point in parameter space that is an ``outlier'' with respect to the posterior
distribution, the first part of the run will be a transient feature where the
MCMC chain (hopefully) moves towards the important regions of the space.
This transient period is called the burn-in, and should usually be excluded
from the sample, otherwise your Monte Carlo summaries will give too much
importance to the part of the space the MCMC happened to go through during
burn-in.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{trace_plot.pdf}
\caption{A ``trace plot''. At the beginning, because of the initial conditions
of the algorithm, the results start in a region of parameter space that has
very low probability. This initial phase is often called the ``burn-in'', and
should be excluded from any subsequent calculations.\label{fig:trace_plot}}
\end{center}
\end{figure}

As an exercise, try running the Metropolis algorithm on the transit problem
with a proposal that is far too small, a proposal that is far too large, and
with an initial condition that is very far from the bulk of the posterior
distribution. Look at the resulting trace plots and
compare them to the healthy one in Figure~\ref{fig:trace_plot}.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{marginal_posterior.pdf}
\caption{The marginal posterior distribution for the parameter $A$ constructed
from MCMC output.\label{fig:marginal_posterior}}
\end{center}
\end{figure}

Another useful type of plot is a scatter plot showing the joint posterior
distribution for a pair of parameters (with all of the other parameters
marginalised out). See Figure~\ref{fig:joint_posterior} for an example.
This allows us to visualise some of the dependences in the remaining
uncertainty about the parameters. For example, there is a slight correlation
between $A$ and $b$ in the posterior, so if we obtained further information
about $A$ from elsewhere, this would affect our knowledge of $b$.
In models with 2-10 interesting parameters, it is common to plot a grid
of such plots, showing each parameter vs each other parameter. These are
sometimes called `corner' or `triangle' plots, and a convenient Python package
for using them is {\tt triangle.py} by Dan
Foreman-Mackey\footnote{Available at
{\tt https://github.com/dfm/triangle.py}}.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{joint_posterior.pdf}
\caption{The joint posterior distribution for $A$ in $b$ constructed from
the MCMC output.\label{fig:joint_posterior}}
\end{center}
\end{figure}


\subsection{Posterior Summaries}
A probability distribution, such as the posterior,
can potentially be arbitrarily complicated. For communication purposes, it is
usually easier to summarise the distribution by a few numbers. The most common
summaries are point estimates (i.e. a single estimate for the value of the
parameter, such as ``we estimate $\theta$ = 0.43'') and intervals (e.g.
the probability that $\theta \in [0.3, 0.5]$ is 68\%).

Thankfully, most of these summaries are trivial to calculate from Monte Carlo
samples, such as those obtained from MCMC. The most popular point estimate is
the posterior mean, which is approximated by the arithmetic mean of the samples.
The posterior median can also be easily approximated by the arithmetic median
of the samples.

\begin{minted}[mathescape,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}
  post_mean = np.mean(samples)
  post_median = np.median(samples)
\end{minted}

There are some theoretical arguments, based on decision theory, that provide
guidance about which point estimate is better under which circumstances.

To compute a credible interval (the Bayesian version of a confidence interval),
you find quantiles of the distribution. For example, if we want to find an
interval that contains 95\% of the posterior probability, the lower end of the
interval is the parameter value for which 2.5\% of the samples are lower.
Similarly, the upper end of the interval is the value for which 2.5\% of the
samples are higher (i.e. 97.5\% are lower). The simplest way to implement this
calculation is by sorting the samples, as shown below.

\begin{minted}[mathescape,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}
  sorted_samples = np.sort(samples)
  # Left and right end of interval
  left = sorted[int(0.025*len(sorted))]
  right = sorted[int(0.975*len(sorted))]
\end{minted}

This kind of interval is sometimes called a {\it centered} credible interval,
because the same amount of probability lies outside the interval on each side.
In astronomy, it is more common to quote a 68\% credible interval, which is
equivalent to ``plus or minus one posterior standard deviation'' if the
posterior is gaussian.

\section{Assigning Prior Distributions}
There are only really two open problems in
Bayesian inference. The first is how to assign sensible prior distributions
(and sampling distributions) in different circumstances, and the
second is how to calculate the results efficiently. With regard to the first
problem ,it is sometimes said that there are two different kinds of Bayesian inference,
{\it subjective} and {\it objective}, with different methods for choosing
priors. My view is that Bayesian describes a {\it hypothetical}
state of prior knowledge, held by an idealised reasoner. When we apply
Bayesian inference, we are studying how the idealised reasoner would update
their state of knowledge based on the information we explicitly put into the
calculation.

In many cases, we can just use simple ``default'' choices for the prior and
the sampling distribution (e.g. a uniform prior for a parameter, and a
``gaussian noise'' assumption for the data). A lot of analyses work just fine
with these assumptions, and taking more care wouldn't change the results in
any important way. However, occasionally it makes sense to spend a lot of
time and effort thinking about the prior distributions.
So-called subjective Bayesians, who are not
necessarily experts in the fields of their clients, conduct elaborate interviews
with experts to try and create a prior that models the experts' beliefs well.
This process is called {\it elicitation}.
For example, consider the Intergovernmental Panel on Climate Change (IPCC),
who periodically write immense reports summarising humanity's state of knowledge
about global warming.
Consider the question ``{\it How much will the global average temperature
increase in the next 100 years?}''. This is exactly the kind of situation where
elicitation of an expert's probabilities is very important: a lot is known,
and a lot is at stake. In this situation, it wouldn't be very wise to rely on
convenient ``vague'' priors!

On the other hand, so-called ``objective'' Bayesians search for principles
based on symmetry or other arguments, which can help choose a prior that is
an appropriate choice to describe a large amount of ignorance. Some examples
include the principle of indifference, the Jeffreys prior, reference priors,
default priors, transformation groups, and entropic priors.

\subsection{Probability distributions have consequences}
When you assign prior distributions, it is more easy than you might expect to
build in an assumption that you don't really agree with. This is especially
true in high dimensional problems, but we'll look at single parameters to start
with.

Imagine that you wanted to infer the mass
$M$ of a galaxy. It might seem reasonable to assume ``prior ignorance''
about $M$, using a uniform distribution
between $10^{5}$ and $10^{15}$ solar masses:
\begin{eqnarray}
M \sim U(10^{5}, 10^{15})
\end{eqnarray}
However, this has an unfortunate feature: it implies that the prior probability
of $M$ being greater than $10^{14}$ is 0.9, and the probability of $M$ being
greater than $10^{12}$ is 0.999, which seems overly confident, when we were
trying to describe ignorance! In astronomy,
we are often in the situation of having to ``put our error bars in the
exponent''. A prior that has this property is the ``log-uniform'' distribution
(named by analogy with the lognormal distribution, and sometimes incorrectly
called a Jeffreys prior), which assigns a uniform distribution to the logarithm
of the parameter. If we replace the uniform prior by
\begin{eqnarray}
\log(M) \sim U(\log(10^{5}), \log(10^{15}))
\end{eqnarray}
then the prior probabilities are more moderate: $P(M > 10^{14}) = 0.1$,
$P(M > 10^{12}) = 0.3$, and so on. The loguniform prior is appropriate for
positive parameters whose uncertainty spans multiple orders of magnitude.
The probability density, in terms of $M$, is proportional to $1/M$.

There are two obvious ways to implement the log-uniform prior in the Metropolis
algorithm. One is to keep $M$ as a parameter, and take the non-uniform prior
into account in the acceptance probability. The other is to treat
$\ell = \log(M)$ as the parameter, in which case the prior is still uniform.
You'll just need to compute $M$ from $\ell$ before you can use it in the
likelihood function. The other way is to keep $M$ as the parameter, but the
prior is then proportional to $1/M$ rather than being constant, so you need
to take into account the prior ratio in the Metropolis acceptance probability.
The first approach (parameterising by $\ell$ instead) is generally a better
idea.

Let's now discuss some consequences of the sampling distribution, for which we
used a normal distribution with known standard deviation for each data point,
and asserted that the measurements were independent. One consequence of this
is a high probability that the noise vector (i.e. all of the actual
the differences between the true curve and the data points) looks
macroscopically like noise, with little correlation between the data points.
To see this, try generating simulated datasets from the sampling distribution
for a particular setting of the parameters, and you will see that almost all
datasets that you generate have this property.

Another implication of our sampling distribution is that we would not expect
very many measurements to depart from the model by more than a few standard
deviations. Normal distributions have very ``light tails'', implying a very
low probability for outliers. If the data set does in fact contain outliers,
the model will try to fit them very closely because the normal distribution
assumption is telling it to believe the data points. This problem also occurs
in least squares fitting, and switching to Bayesian inference will only resolve
the problem if we use something other than a normal distribution for the
sampling distribution. See \citet{hoggline} for a detailed discussion of this
issue.


\section{What is the data?}
It can sometimes be helpful to think carefully about exactly what your data set
contains, and whether it is all ``data'' in the sense of Bayesian inference.
In the transit example, we chose a sampling distribution for the data, but this
sampling distribution was for the $y$-values of the data
(the flux measurements). Note that we never assigned a probability distribution
for the {\it times} of the data points, even though we might think of that as
part of the data (it's probably in the same file as the measurements, after all).
In fact, because we didn't assign a probability distribution for it, the
timestamps were not data at all, but part of the prior information $I$.
Therefore, it is completely justifiable to use the timestamps when assigning
the prior and the sampling distribution. It is not ``cheating'', even slightly,
to use the time information as we did to set the priors.

However, if we had used the $y$-values in the dataset to help choose the prior
(perhaps for $A$), this would have been technically incorrect. Nevertheless,
it is common practice to look at the data before assigning priors, and the
real test of the legitimacy of this practice is whether it makes any difference
to your results. In the transit example, if we had used a $U(-50, 50)$ prior
instead of a $U(-100, 100)$ one, the posterior distribution would have been
virtually the same. The only substantial difference would have been in the
value of the marginal likelihood, had we calculated it
(see Section~\ref{sec:model_selection} for more information about when this
is important).



\section{Birth and death moves}
Sometimes, when you are trying to fit a model to data, there is uncertainty
not just about the values of the model parameters, but also how many model
parameter exist in the first place. One example is
the radial velocity technique for detecting extra-solar planets
\citep{gregory}, where the goal is to fit a signal to a sparse data set, where
we don't know how many components (planets) there are. Other examples are
searching for emission lines in a noisy spectrum, and searching for faint
sources in a noisy image.



\section{Model Selection and Nested Sampling}\label{sec:model_selection}
Nested Sampling is a Monte Carlo algorithm introduced by \citet{skilling}. It
is not technically an MCMC algorithm, although MCMC can be used as part of the
implementation. It has some advantages over related Monte Carlo methods
such as parallel tempering \citep{pt, gregory, tempering}, but
also brings with it its own challenges.
Several variants of Nested Sampling exist, and most of them are somewhat
complicated. Here, I will present a simple version of Nested Sampling which
is sufficient to solve a wide range of problems. This approach is similar to
the one presented in the introductory textbook by \citet{sivia}.

Many more complex and sophisticated versions of
Nested Sampling exist, such as the popular MultiNest \citep{multinest},
my own Diffusive Nested Sampling \citep{dnest}, and several others. These
algorithms, while all based on the insights of \citet{skilling}, are very
different in detail. See the appendix for more details.

Consider two different models $M_1$ and $M_2$ which are mutually exclusive
(they can't both be true).
Suppose $M_1$ has parameters
$\btheta_1$, and $M_2$ has its own parameters $\btheta_2$. The methods
described previously can be used to calculate the posterior distribution for
$M_1$'s parameters:
\begin{eqnarray}
p(\btheta_1 | D, M_1) &=& \frac{p(\btheta_1 | M_1)p(D | \btheta_1, M_1)}
{p(D | M_1)}
\end{eqnarray}
You can also fit model $M_2$ to the data, i.e. get the posterior distribution
for $M_2$'s parameters:
\begin{eqnarray}
p(\btheta_2 | D, M_2) &=& \frac{p(\btheta_1 | M_2)p(D | \btheta_2, M_2)}
{p(D | M_2)}
\end{eqnarray}
This is all very well, but you might want to know whether $M_1$ or $M_2$ is
more plausible overall, given your data. That is, you want the posterior
probabilities $P(M_1 | D)$ and $P(M_2 | D)$.

In this situation, it's usually easier to calculate the ratio of the two
posterior probabilities, which is sometimes called the {\it posterior odds
ratio}:
\begin{eqnarray}
\frac{P(M_2 | D)}{P(M_1 | D)} &=& \frac{P(M_2)}{P(M_1)}
\times \frac{P(D | M_2)}{P(D | M_1)}
\end{eqnarray}

As you might expect, the posterior odds for $M_2$ over $M_1$ depends on the
prior odds: was $M_2$ more plausible than $M_1$ before taking into account the
data? The other ratio is a ratio of likelihoods, how probable was the data
assuming $M_2$ vs assuming $M_1$? These likelihoods aren't the likelihoods
for a specific value of the parameters, but instead likelihoods for the model
as a whole. To distinguish this kind of likelihood from the standard kind (which
is a function of the model parameters), the term {\it marginal likelihood} or
{\it evidence} is used. The marginal likelihood for $M_1$ is:
\begin{eqnarray}
p(D | M_1) &=& \int p(\btheta_1 | M_1) p(D | \btheta_1, M_1) \, d^{N_1} \btheta_1
\label{eq:marginal_likelihood}
\end{eqnarray}
To do model selection, we need this as well as the marginal likelihood for
$M_2$:
\begin{eqnarray}
p(D | M_2) &=& \int p(\btheta_2 | M_2) p(D | \btheta_2, M_2) \, d^{N_2} \btheta_2
\end{eqnarray}
Marginal likelihoods are integrals over the parameter space. They are
expected values, but they are not expected values with respect to the posterior
distribution, but rather the prior. Therefore, we cannot use standard MCMC
methods (at least in any simple way) to calculate the marginal
likelihood\footnote{There is a method, called the ``harmonic mean estimator'',
for estimating the marginal likelihood from posterior samples. Bayesian
statistician Radford Neal has described it as the
``worst Monte Carlo method ever''.}.

A Monte Carlo approach that samples from the prior distribution will also fail
in most cases. The likelihood function $p(D | \btheta_1, M_1)$ in
Equation~\ref{eq:marginal_likelihood} is usually sharply peaked in a very
small region of parameter space. The integral will be dominated by the high values
of the likelihood in the tiny region, yet a Monte Carlo approach based on
sampling the prior will almost certainly not give any samples in the important
region.

\section{An easy problem}
Imagine a parameter estimation problem with a single parameter $x$ with a
uniform prior distribution between 0 and 1. If the likelihood function was a
decreasing function of $x$ (like in Figure~\ref{fig:nested1}). In this
situation the marginal likelihood would be a straightforward integral
$Z = \int_0^1 L(x) \,dx$. If we could obtain some points $x$, and measure their
likelihoods $L$, we could approximate the integral numerically, using the
trapezoidal rule or some other numerical quadrature method.
The key insight of Nested Sampling is that any high dimensional Bayesian
problem can be mapped into a one dimensional problem like that shown in
Figure~\ref{eq:nested1}.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{nested1.pdf}
\caption{\label{fig:nested1}}
\end{center}
\end{figure}



\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{nested2.pdf}
\caption{\label{fig:nested2}}
\end{center}
\end{figure}


\section{Appendix: Notation for Probability Distributions}
There are two main types of notation that are used in probability theory. The
first type is mathematically more correct and conceptually
clear, yet results in equations that are very large, complicated,
and difficult to read.
The second type is a very compact notation which makes equations a lot
shorter and easier to read, but could be misinterpreted if you are not used to
it. I prefer the compact notation, and it is more popular in the Bayesian
literature. To prevent any misunderstandings I'll describe both types of
notation here.

Suppose we're about to flip a coin $N$ times, and are interested in the
quantity $X$, the number of times the result is heads. According
to the standard assumptions, the possible values for $X$ are the integers from 0
to 10, and the probability distribution for $X$ is given by
a binomial distribution:

\begin{eqnarray}
P(X = x) &=& \left(\begin{array}{cc}10\\ x\end{array}\right)
\left(\frac{1}{2}\right)^x\left(\frac{1}{2}\right)^{10 - x}\nonumber\\
&=& \left(\begin{array}{cc}10\\ x\end{array}\right)
\left(\frac{1}{2}\right)^{10}\label{eq:binomial1}
\end{eqnarray}
The equation gives us all the probabilities we want, i.e. $P(X=0)$, $P(X=1)$,
and so on. The lower case $x$ is a dummy variable: like the index in a sum, it
can be replaced by any other symbol and the equation still holds. For example,
replacing $x$ with $a$ gives:
\begin{eqnarray}
P(X = a) &=& \left(\begin{array}{cc}10\\ a\end{array}\right)
\left(\frac{1}{2}\right)^{10}\label{eq:binomial2}
\end{eqnarray}
which has exactly the same meaning as Equation~\ref{eq:binomial1}.

The compact notation is an alternative to the left hand sides of
Equations~\ref{eq:binomial1} and~\ref{eq:binomial2}.
\begin{eqnarray}
p(x) &=& \left(\begin{array}{cc}10\\ x\end{array}\right)
\left(\frac{1}{2}\right)^{10}
\end{eqnarray}
Since $X$, the actual number of heads, isn't written anywhere, we forget it
exists and just use the lower case $x$ for that purpose, even though it was
originally a dummy variable!
The key to understanding this notation is to read $p(x)$ as ``the probability
distribution for $x$'', and to understand what the expression following it
really means. If the set of possible $x$ values is continuous (so $p(x)$ gives
the probability {\it density} instead of a probability itself) then the
notation doesn't change.

The compact notation is especially helpful in Bayesian statistics because
we deal with conditional probability distributions a lot. For example, if we
have three variables $a$, $b$, and $c$, the following is a true statement
(it's an example of the {\it product rule} of probabilities):
\begin{eqnarray}
p(a, b, c) &=& p(a)p(b|a)p(c|b,a).
\end{eqnarray}
Written in the non-compact notation, we'd have three variables $A$, $B$, and
$C$, and the corresponding equation would be
\begin{eqnarray}
P(A=a | B=b, C=c) &=&
 P(A=a)P(B=b|A=a)\\
& & \times P(C=c|B=b,A=a)
\end{eqnarray}
which is much harder to read and manipulate.

\section{Appendix: A rough guide to popular Bayesian computation packages}
This is a very brief list of popular Bayesian computation software packages
that you might find useful. This is far from an exhaustive list; it only
includes packages that I know something about. I apologise if your favourite
package is not listed here.

{\bf MultiNest} \citep{multinest} is one of the most popular implementations of
Nested Sampling. It does not use MCMC, like the version presented in this
chapter. It is designed to handle potentially multimodal posterior
distributions in low ($\lesssim 30$) dimensions. It is particularly useful in
situations where the likelihood function is expensive to evaluate.

{\bf Emcee} \citep{emcee} is useful for getting quick, efficient results on low
dimensional ($\lesssim 30$) unimodal posterior distributions.
It is written in Python, and one of its main advantages is that the user only
needs to write a function that evaluates the log of the posterior density.

{\bf JAGS} \citep{jags}: The main advantage of JAGS is that it
uses the BUGS language, a neat way of specifying your modelling assumptions
using a language similar to the ``$\sim$'' notation. This makes JAGS very
suitable for quickly implementing analyses of small to medium complexity without
having to worry too much about MCMC algorithms themselves. For a very gentle
introduction to JAGS you can consult my lecture notes at
{\tt www.github.com/eggplantbren/STATS331}.

{\bf Stan} \citep{stan}: The modelling language of Stan is similar to that of
JAGS, although the underlying MCMC methods used are completely different. Stan
is based on the No U-Turn Sampler (NUTS), a variant of the Hamiltonian MCMC
method \citep{hamiltonian}. This is a very efficient sampler in problems with
a continuous parameter space and hierarchically defined priors.

{\bf DNest} \citep{dnest}: This is another version of Nested Sampling, but it
is quite different from MultiNest. It uses MCMC, but rather than embedding
MCMC in the standard Nested Sampling framework as we did in this chapter, it
uses MCMC to explore a target distribution which is inspired by Nested Sampling.
DNest is most useful on problems with large numbers of parameters and/or a
complicated posterior distribution, but where the
likelihood function can be evaluated quite quickly. The main downside is that
you need to write a fair bit of C++ code in order to implement a new model.

\section{Acknowledgements}
I would like to thank the members of the `Astrostatistics' Facebook group for
answering my query about MultiNest.

\begin{thebibliography}{999}
\bibitem[\protect\citeauthoryear{Brewer, P{\'a}rtay,
\& Cs{\'a}nyi}{2011}]{dnest} Brewer B.~J., P{\'a}rtay L.~B., Cs{\'a}nyi G., 2011,
Statistics and Computing, 21, 4, 649-656. arXiv:0912.2380

\bibitem[\protect\citeauthoryear{Feroz, Hobson,
\& Bridges}{2009}]{multinest} Feroz F., Hobson M.~P., Bridges M., 2009, MNRAS, 398, 1601

\bibitem[\protect\citeauthoryear{Foreman-Mackey et al.}{2012}]{emcee} Foreman-Mackey, 
D., Hogg, D.~W., Lang, D., \& Goodman, J.\ 2012, emcee: The MCMC Hammer, arXiv:1202.3665 

\bibitem[\protect\citeauthoryear{Homan and Gelman}{2014}]{nuts}
Homan, Matthew D., and Andrew Gelman, 2014, ``The no-U-turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo.'' The Journal of Machine Learning Research 15, no. 1 (2014): 1593-1623.

\bibitem[\protect\citeauthoryear{Gelman}{2013}]{gelman}
Gelman, Andrew, John B. Carlin, Hal S. Stern, and Donald B. Rubin. Bayesian data analysis, third edition. Chapman \& Hall/CRC, 2013.

\bibitem[\protect\citeauthoryear{Gregory}{2005}]{gregory}
Gregory, Phil. Bayesian Logical Data Analysis for the Physical Sciences: A Comparative Approach with Mathematica® Support. Cambridge University Press, 2005.

\bibitem[\protect\citeauthoryear{Hansmann}{1997}]{pt} Hansmann, Ulrich HE.,
1997, Parallel tempering algorithm for conformational studies of biological
molecules., Chemical Physics Letters 281, no. 1 (1997): 140-150.

\bibitem[Hogg et al.(2010)]{hoggline} Hogg, D.~W., Bovy, J., 
Lang, D.\ 2010.\ Data analysis recipes: Fitting a model to data.\ ArXiv 
e-prints arXiv:1008.4686. 

\bibitem[\protect\citeauthoryear{Mackay}{2003}]{mackay} Mackay,
D. J. C.\ 2003, Information theory, Inference, and Learning
Algorithms, Cambridge University Press

\bibitem[\protect\citeauthoryear{Neal}{2011}]{hamiltonian}
Neal, Radford M., 2011. ``MCMC using Hamiltonian dynamics.''
Handbook of Markov Chain Monte Carlo 2.

\bibitem[\protect\citeauthoryear{Plummer}{2003}]{jags}
Plummer, M., 2003, JAGS: A Program for Analysis of Bayesian Graphical Models
Using Gibbs Sampling, Proceedings of the 3rd International Workshop on
Distributed Statistical Computing (DSC 2003), March 20-22, Vienna, Austria.
ISSN 1609-395X.

\bibitem[\protect\citeauthoryear{O'Hagan and Forster}{2004}]{ohagan}
O'Hagan, A., Forster,~J., 2004, Bayesian inference. London: Arnold.

\bibitem[\protect\citeauthoryear{Sivia and Skilling}{2006}]{sivia} Sivia, 
D.~ S., Skilling, J., 2006, Data Analysis: A Bayesian Tutorial, 2nd 
Edition, Oxford University Press

\bibitem[\protect\citeauthoryear{Skilling}{2006}]{fred} Skilling, 
J., 2006, ``Nested Sampling for General Bayesian Computation'', Bayesian 
Analysis 4, pp. 833-860.

\bibitem[\protect\citeauthoryear{Stan Development Team}{2014}]{stan} Stan Development Team, 2014,
Stan: A C++ Library for Probability and Sampling, Version 2.5.0
{\tt http://mc-stan.org/}

\bibitem[\protect\citeauthoryear{Vousden et al.}{2015}]{tempering} Vousden, W., Farr, 
W.~M., Mandel, I. 2015. Dynamic temperature selection for 
parallel-tempering in Markov chain Monte Carlo simulations. ArXiv e-prints 
arXiv:1501.05823. 

\end{thebibliography}


%Bla bla bla \citep{lecturer1:abreu10}, \citep{lecturer1:abreu100}

%\input lecturer1/lecturer1.bbl

