\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\data}{\boldsymbol{D}}

\author[Lecturer1]{Brendon J. Brewer\\
Department of Statistics, The University of Auckland}

\chapter{Bayesian Inference How-To}

\section{Introduction}


Any particular application of Bayesian inference involves making choices
about what data you are analysing, what questions you are
asking, and what assumptions you are willing to make. Once you have made all
of these choices, you are then faced with the question of how to calculate the
results. Often, this involves numerical methods. The most powerful and
popular numerical techniques are the Markov Chain Monte Carlo methods, often
abbreviated as MCMC.

There is a huge variety of Markov Chain Monte Carlo methods, and it would be
unwise to try to cover them all in this winter school. Therefore I will focus
on a small number of methods that are simple to implement, yet quite powerful
and widely applicable. At times, I will comment on popular methods that we are
not covering, to give you some idea about what's out there.


\section{Notation}
People who use probability theory tend to fall into one of two camps. The
first group favours notation that is mathematically correct and conceptually
clear, yet results in equations that are very large and difficult to read.
The second group favours a very compact notation which makes equations a lot
shorter and easier to read, but could be misinterpreted if you are not used to
it. I fall into the second group, but to prevent any misunderstandings
I'll describe these two kinds of notation below.

Suppose we're about to flip a coin $N$ times, and are interested in the
quantity $X$, the number of times the result is heads. According
to the usual assumptions, the possible values for $X$ are the integers from 0
to 10, and the probability distribution for $X$ is given by
a binomial distribution:

\begin{eqnarray}
P(X = x) &=& \left(\begin{array}{cc}10\\ x\end{array}\right)
\left(\frac{1}{2}\right)^x\left(\frac{1}{2}\right)^{10 - x}
\end{eqnarray}



\section{Parameter Estimation}

There are some parameters $\btheta$ whose values we want to know. To start, we
need to have some idea of the set of possible values we are considering. For
example, are the parameters integers? Real numbers? Positive real numbers?
In some examples, the definition of the parameters already restricts the set
of possible values. For example, {\it the proportion of extra-solar planets in
the Milky Way that contain life} cannot be less than 0 or greater than 1.

Our uncertainty about the parameters is modelled by a prior distribution
$p(\btheta | I)$.

Bayes' rule is a consequence of the product rule of probability. If we have
chosen a certain prior distribution $p(\btheta | I)$, and a certain sampling
distribution $p(\data | \btheta, I)$, then Bayes' rule allows us to calculate
the {\it posterior distribution}:
\begin{eqnarray}
p(\btheta | \data, I) &=& \frac{p(\btheta | I)p(\data | \btheta, I)}{p(\data | I)}.
\end{eqnarray}



\section{What is the data?}



\section{Assigning Prior Distributions}
It has been said ({\bf by whom?}) that there are only two open problems in
Bayesian inference. The first is how to assign prior distributions, and the
second is how to calculate the results.

It is sometimes said that there are two different kinds of Bayesian inference,
{\it subjective} and {\it objective}, with different methods for choosing
priors. My view is that Bayesian describes a {\it hypothetical}
state of prior knowledge, held by an idealised reasoner. When we apply
Bayesian inference, we are studying how the idealised reasoner would update
their state of knowledge based on the information we use in the calculation.

In many cases, we can just use simple choices of wide prior distributions,
invoking ``ignorance''.

In some problems, it makes sense to spend a lot of time and effort thinking
about the prior distributions. Some Bayesian statisticians, who are not
necessarily experts in the fields of their clients, conduct elaborate interviews
with experts to try and create a prior that models the experts' beliefs well.
This process is called {\it elicitation}.
For example, consider the Intergovernmental Panel on Climate Change (IPCC).
Every XX years, they write a long and detailed report about the latest climate
change science, trying to summarise everything that humanity knows about the
topic. Consider the question ``{\it How much will the global average temperature
increase in the next 100 years if we continue along our present path?}''.
In this situation, it wouldn't be a very good idea to just use a convenient
uniform prior!




\section{The Metropolis Algorithm}


The version of the Metropolis algorithm presented here is sometimes called
{\it random walk} Metropolis, because of the choice of the proposal
distribution. More sophisticated choices are often possible, but usually
require problem-specific knowledge. Consider a problem with a single unknown
parameter $x$. If the prior is some density $\pi(x)$ and the likelihood
function is $\mathcal{L}(x)$, then the posterior distribution will be
proportional to $\pi(x)\mathcal{L}(x)$.

The ``random walk'' proposal generates a proposed value $x'$ from a normal
(gaussian) distribution centered around the current position $x$. The user
is free to choose the width of the normal distribution. Here is a Python code
snippet showing a proposal with width {\tt L}:

\begin{minted}[mathescape,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}
  # Generate a proposal
  proposal = x + L*rng.randn()
\end{minted}

The performance of the Metropolis algorithm seems to depend quite strongly on
the width of the proposal distribution. It's usually not a good idea to spend
a long time on preliminary runs to find an optimal width. Instead, I recommend
that you use a mixture of widths. Basically, every time we make a proposal,
the width is drawn from some range, rather than being constant.
\begin{minted}[mathescape,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}
  # A heavy-tailed proposal distribution
  # Generate a standard deviation
  L = 10.**(1.5 - 6.*rng.rand())

  # Use the standard deviation for the proposal
  proposal = x + L*rng.randn()
\end{minted}

With this proposal, the minimum width is
$10^{-4.5} \approx 3.16 \times 10^{-5}$, and the maximum width is
$10^{1.5} \approx 31.6$.

The effective proposal distribution is now very heavy-tailed.



\section{Birth and death moves}
Sometimes, when you are trying to fit a model to data, there is uncertainty
not just about the values of the model parameters, but also how many model
parameter exist in the first place. One example is
the radial velocity technique for detecting extra-solar planets
\citep{gregory}, where the goal is to fit a signal to a sparse data set, where
we don't know how many components (planets) there are.



\section{Nested Sampling}
Nested Sampling is a Monte Carlo algorithm introduced by \citet{skilling}.
Several variants of Nested Sampling exist, and most of them are somewhat
complicated. Here, I will present a simple version of Nested Sampling which
is sufficient to solve a wide range of problems. This approach is similar to
the one presented in the introductory textbook by \citet{sivia}.

Many more complex and sophisticated versions of
Nested Sampling exist, such as the popular MultiNest \citep{multinest},
my own Diffusive Nested Sampling \citep{dnest}, and several others. These
algorithms, while all based on the insights of \citet{skilling}, are very
different in detail. For example, MultiNest does blah, while Diffusive Nested
Sampling does blah'.


%Bla bla bla \citep{lecturer1:abreu10}, \citep{lecturer1:abreu100}

%\input lecturer1/lecturer1.bbl

